# -*- coding: utf-8 -*-
import json
import os
import time
from tqdm import tqdm
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from config import Config
from modules.vector_store import VectorStoreManager
from modules.prompts import RAGPrompts
from loguru import logger

class NERProcessor:
    def __init__(self, vector_store_path, index_name="index"):
        self.index_name = index_name
        self.vector_store_path = vector_store_path
        self.vec_manager = VectorStoreManager()
        
        logger.info("ğŸ”„ åŠ è½½å‘é‡åº“,NERä»»åŠ¡æ— éœ€é‡‡ç”¨embeddingæ¨¡å¼...")
        # 1. åŠ è½½å‘é‡åº“ (æ­¤æ—¶å®ƒæ˜¯ç¡¬ç›˜ä¸Šçš„æ—§çŠ¶æ€)
        self.vector_store = self.vec_manager.load(
            vector_store_path=self.vector_store_path,
            index_name=self.index_name,
            use_embedding_model=False
        )
        
        # 2. å®šä¹‰æ–‡ä»¶è·¯å¾„
        self.checkpoint_file = os.path.join(Config.DOCS_DIR, "ner_checkpoint.json")
        self.cache_file = os.path.join(Config.DOCS_DIR, "ner_temp_cache.json")

        # 3. åˆå§‹åŒ–çŠ¶æ€ (æ ¸å¿ƒé€»è¾‘)
        # committed_ids: å·²ç»ç¡®ä¿å­˜å…¥ FAISS æ–‡ä»¶çš„ ID
        # cached_data:   å·²ç»è¯†åˆ«å®Œä½†åªå­˜åœ¨ json é‡Œçš„ä¸´æ—¶æ•°æ® {doc_id: ner_result}
        self.committed_ids, self.cached_data = self._load_state()

        # 4. å°†ç¼“å­˜ä¸­çš„æ•°æ®â€œå›æ”¾â€åˆ°å†…å­˜ä¸­çš„å‘é‡åº“
        # è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼šè™½ç„¶ FAISS æ–‡ä»¶æ²¡å­˜ï¼Œä½†æˆ‘ä»¬æŠŠä¸Šæ¬¡å´©æºƒå‰ç¼“å­˜çš„ NER ç»“æœé‡æ–°æ³¨å…¥å†…å­˜
        if self.cached_data:
            logger.info(f"ğŸ”„ æ­£åœ¨å›æ”¾ {len(self.cached_data)} æ¡ç¼“å­˜æ•°æ®åˆ°å†…å­˜å‘é‡åº“...")
            self._apply_cache_to_memory(self.cached_data)

        # 5. åˆå§‹åŒ–æ¨¡å‹ (ä¿æŒä¸å˜)
        if not Config.LOCAL_LLM_SERVICE_PATH:
            raise ValueError("è¯·é…ç½®Ollama LOCAL_LLM_SERVICE_PATH")
            
        self.local_llm = ChatOllama(
            model="qwen2.5:1.5b-instruct", 
            temperature=0.1, # NER ä»»åŠ¡å»ºè®®ä½æ¸©
            format="json", 
            base_url="http://127.0.0.1:11434",
            timeout=60,
            num_predict=512
        )
        self.parser = JsonOutputParser()
        self.prompt = PromptTemplate(template=RAGPrompts.NER_TEMPLATE, input_variables=["text"])
        self.chain = self.prompt | self.local_llm | self.parser

    def _load_state(self):
        """
        å¯åŠ¨æ—¶è¯»å–çŠ¶æ€
        Returns:
            committed_ids (set): å‘é‡åº“ä¸­å·²å›ºåŒ–çš„
            cached_data (dict): ä¸´æ—¶æ–‡ä»¶ä¸­çš„ {id: result}
        """
        committed = set()
        cached = {}

        # è¯»å– Checkpoint (è®°å½• ID åˆ†ç»„)
        if os.path.exists(self.checkpoint_file):
            try:
                with open(self.checkpoint_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    committed = set(data.get("committed_ids", []))
                    # cached_ids = set(data.get("cached_ids", [])) # å…¶å®ä¸éœ€è¦è¯»è¿™ä¸ª listï¼Œç›´æ¥è¯» cache æ–‡ä»¶æ›´å‡†
            except Exception as e:
                logger.error(f"è¯»å– Checkpoint å¤±è´¥: {e}")

        # è¯»å– Cache Data (è®°å½•å®é™… NER å†…å®¹)
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cached = json.load(f)
            except Exception as e:
                logger.error(f"è¯»å–ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
        
        return committed, cached

    def _apply_cache_to_memory(self, cache_dict):
        """å°†ç¼“å­˜æ•°æ®æ³¨å…¥å½“å‰å†…å­˜ä¸­çš„ VectorStore"""
        docstore = self.vec_manager.get_documents(self.vector_store)
        if not docstore:
            logger.error("å‘é‡åº“å®ä¾‹ä¸ºNoneï¼Œæ— æ³•å®ç°æ•°æ®neræ›´æ–°ã€‚")
            return
        
        count = 0
        for doc_id, ner_result in cache_dict.items():
            if doc_id in docstore:
                docstore[doc_id].metadata["ner"] = ner_result
                count += 1
        logger.info(f"âœ… å·²æ¢å¤ {count} æ¡ç¼“å­˜è®°å½•åˆ°å†…å­˜ã€‚")

    def _save_temp_state(self):
        """
        ã€å°æ­¥é¢‘ä¿å­˜ã€‘
        åªä¿å­˜ checkpoint å’Œ cache.jsonï¼Œä¸ç¢° FAISS
        é€Ÿåº¦å¿«ï¼Œå¼€é”€å°
        """
        try:
            # 1. ä¿å­˜ NER ç»“æœå†…å®¹
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cached_data, f, ensure_ascii=False)
            
            # 2. ä¿å­˜ ID çŠ¶æ€
            state = {
                "committed_ids": list(self.committed_ids),
                "cached_ids": list(self.cached_data.keys()) # è¿™äº›æ˜¯å·²å¤„ç†ä½†æœªå…¥åº“çš„
            }
            with open(self.checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(state, f)
                
            # logger.debug(f"âš¡ ä¸´æ—¶çŠ¶æ€å·²ä¿å­˜ (ç¼“å­˜å¤§å°: {len(self.cached_data)})")
        except Exception as e:
            logger.error(f"ä¿å­˜ä¸´æ—¶çŠ¶æ€å¤±è´¥: {e}")

    def _save_full_state(self):
        """
        ã€å¤§æ­¥é¢‘ä¿å­˜ã€‘
        ä¿å­˜ FAISSï¼Œæ¸…ç©ºç¼“å­˜ï¼Œç§»åŠ¨ ID çŠ¶æ€
        """
        try:
            logger.info("ğŸ’¾ æ­£åœ¨æ‰§è¡Œå…¨é‡æŒä¹…åŒ– (å†™å…¥å‘é‡åº“)...")
            
            # 1. ä¿å­˜ FAISS (æœ€æ…¢çš„ä¸€æ­¥)
            self.vec_manager.save_updated_store(self.vector_store, self.vector_store_path, self.index_name)
            
            # 2. çŠ¶æ€è½¬ç§»ï¼šCache -> Committed
            # å› ä¸ºæ•°æ®å·²ç»è¿› FAISS æ–‡ä»¶äº†ï¼Œæ‰€ä»¥ cached_data å¯ä»¥æ¸…ç©º
            self.committed_ids.update(self.cached_data.keys())
            self.cached_data = {} # æ¸…ç©ºå†…å­˜ç¼“å­˜
            
            # 3. æ¸…ç†/æ›´æ–°ç£ç›˜ä¸Šçš„ä¸´æ—¶æ–‡ä»¶
            self._save_temp_state() # è¿™ä¼šæŠŠç©ºçš„ cache å†™å…¥ç£ç›˜ï¼Œå¹¶æ›´æ–° committed_ids
            
            logger.info("âœ… å…¨é‡ä¿å­˜å®Œæˆï¼Œç¼“å­˜å·²æ¸…ç©ºã€‚")
        except Exception as e:
            logger.error(f"âŒ å…¨é‡ä¿å­˜å¤±è´¥: {e}")

    def run(self, batch_size=4, small_step=100, big_step=2000):
        docstore = self.vec_manager.get_documents(self.vector_store)
        if not docstore:
            return

        # è®¡ç®—å¾…å¤„ç†ä»»åŠ¡
        # å¾…å¤„ç† = æ€»æ–‡æ¡£ - (å·²å…¥åº“ + åœ¨ç¼“å­˜ä¸­)
        processed_ids = self.committed_ids.union(self.cached_data.keys())
        pending_items = [(k, v) for k, v in docstore.items() if k not in processed_ids]
        
        total = len(pending_items)
        if total == 0:
            logger.info("æ‰€æœ‰æ–‡æ¡£å‡å·²å®Œæˆå¤„ç†ã€‚")
            return

        logger.info(f"ğŸš€ å¼€å§‹ä»»åŠ¡ | å¾…å¤„ç†: {total} | å·²å…¥åº“: {len(self.committed_ids)} | ç¼“å­˜ä¸­: {len(self.cached_data)}")
        
        counter = 0 # ä»…ç”¨äºæœ¬æ¬¡è¿è¡Œçš„è®¡æ•°

        with tqdm(total=total, desc="NERå¤„ç†ä¸­") as pbar:
            for i in range(0, total, batch_size):
                batch_items = pending_items[i : i + batch_size]
                
                texts = [doc.page_content for _, doc in batch_items]
                ids = [doc_id for doc_id, _ in batch_items]

                try:
                    # 1. LLM æ¨ç†
                    results = self.chain.batch([{"text": t} for t in texts])
                    
                    # 2. å†…å­˜æ›´æ–° (VectorStore + CacheDict)
                    for doc_id, ner_result in zip(ids, results):
                        if ner_result:
                            # A. æ›´æ–°åˆ°å†…å­˜ VectorStore (ä¸ºäº†æ£€ç´¢èƒ½ç«‹åˆ»ç”¨åˆ°ï¼Œä¹Ÿä¸ºäº†æœ€ç»ˆ save)
                            docstore[doc_id].metadata["ner"] = ner_result
                            
                            # B. æ›´æ–°åˆ°å†…å­˜ CacheDict (ä¸ºäº†å°æ­¥é¢‘å­˜ç›˜)
                            self.cached_data[doc_id] = ner_result
                    
                    current_batch_len = len(batch_items)
                    counter += current_batch_len
                    pbar.update(current_batch_len)

                    # 3. æ£€æŸ¥ä¿å­˜ç­–ç•¥
                    
                    # è§¦å‘å¤§æ­¥é¢‘ (è½åº“)
                    if counter % big_step < batch_size and counter > 0:
                        self._save_full_state()
                    
                    # è§¦å‘å°æ­¥é¢‘ (å­˜ç¼“å­˜)
                    elif counter % small_step < batch_size:
                        self._save_temp_state()
                        # logger.info(f"ğŸš€ å¼€å§‹ä»»åŠ¡ | å¾…å¤„ç†: {total} | å·²å…¥åº“: {len(self.committed_ids)} | ç¼“å­˜ä¸­: {len(self.cached_data)}")

                except Exception as e:
                    logger.error(f"Batch Error: {e}")

        # å¾ªç¯ç»“æŸåçš„æœ€ç»ˆä¿å­˜
        self._save_full_state()


    def run_chuanliu(self, batch_size=1, small_step=100, big_step=2000):
            """
            æ”¹å†™åçš„å•æ¡å¤„ç†æ¨¡å¼
            batch_size å»ºè®®è®¾ä¸º 1 ä»¥ä¾¿ç²¾ç»†åŒ–æ’æŸ¥
            """
            docstore = self.vec_manager.get_documents(self.vector_store)
            if not docstore:
                return

            # 1. è®¡ç®—å¾…å¤„ç†ä»»åŠ¡
            processed_ids = self.committed_ids.union(self.cached_data.keys())
            pending_items = [(k, v) for k, v in docstore.items() if k not in processed_ids]
            
            total = len(pending_items)
            if total == 0:
                logger.info("æ‰€æœ‰æ–‡æ¡£å‡å·²å®Œæˆå¤„ç†ã€‚")
                return

            # 2. å‡†å¤‡å¤±è´¥è®°å½•æ–‡ä»¶
            failed_log_path = os.path.join(Config.DOCS_DIR, "ner_failed_ids.txt")

            logger.info(f"ğŸš€ ä¸²è¡Œæ¨¡å¼å¯åŠ¨ | å¾…å¤„ç†: {total} | å·²å…¥åº“: {len(self.committed_ids)} | ç¼“å­˜ä¸­: {len(self.cached_data)}")
            
            counter = 0 

            with tqdm(total=total, desc="NERå¤„ç†ä¸­") as pbar:
                # æ³¨æ„ï¼šå³ä¾¿è¿™é‡Œä¼ äº† batch_size > 1ï¼Œå†…éƒ¨ä¹Ÿä¼šé€æ¡å¤„ç†ä»¥ç¡®ä¿å®‰å…¨
                for i in range(0, total, batch_size):
                    batch_items = pending_items[i : i + batch_size]
                    
                    for doc_id, doc in batch_items:
                        text = doc.page_content
                        try:
                            # --- æ ¸å¿ƒä¿®æ”¹ï¼šå•æ¡æ¨ç† ---
                            # å¦‚æœåœ¨ ChatOllama åˆå§‹åŒ–æ—¶è®¾ç½®äº† timeoutï¼Œè¿™é‡Œä¼šç”Ÿæ•ˆ
                            ner_result = self.chain.invoke({"text": text})
                            
                            if ner_result:
                                # A. æ›´æ–°å†…å­˜å‘é‡åº“
                                docstore[doc_id].metadata["ner"] = ner_result
                                # B. æ›´æ–°ä¸´æ—¶ç¼“å­˜å­—å…¸
                                self.cached_data[doc_id] = ner_result
                            else:
                                logger.warning(f"âš ï¸ ID: {doc_id} è¿”å›ç»“æœä¸ºç©º")

                        except Exception as e:
                            # --- æ ¸å¿ƒä¿®æ”¹ï¼šè®°å½•å¤±è´¥ ID ---
                            logger.error(f"âŒ å¤„ç†å¤±è´¥ | ID: {doc_id} | é”™è¯¯: {e}")
                            with open(failed_log_path, "a", encoding="utf-8") as f:
                                f.write(f"{doc_id}\n")
                            # å¤±è´¥åç»§ç»­ä¸‹ä¸€æ¡ï¼Œä¸ä¸­æ–­ç¨‹åº
                            continue

                        finally:
                            counter += 1
                            pbar.update(1)

                        # 3. æ£€æŸ¥ä¿å­˜ç­–ç•¥ (ç§»åŠ¨åˆ°å•æ¡å¾ªç¯å†…ï¼Œä¿è¯æ­¥é¢‘å‡†ç¡®)
                        # è§¦å‘å¤§æ­¥é¢‘ (è½åº“ FAISS)
                        if counter > 0 and counter % big_step == 0:
                            self._save_full_state()
                            logger.info(f"ğŸ’¾ å·²å®Œæˆç¬¬ {counter} æ¡çš„å¤§æ­¥é¢‘å…¨é‡ä¿å­˜")
                        
                        # è§¦å‘å°æ­¥é¢‘ (å­˜ JSON ç¼“å­˜)
                        elif counter > 0 and counter % small_step == 0:
                            self._save_temp_state()

            # å¾ªç¯å½»åº•ç»“æŸåçš„æœ€ç»ˆä¿å­˜
            self._save_full_state()
            logger.info("ğŸ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ã€‚")

if __name__ == "__main__":
    ner = NERProcessor(Config.VECTOR_STORE_PATH)
    # å°æ­¥é¢‘ 100 å­˜ä¸€æ¬¡ jsonï¼Œå¤§æ­¥é¢‘ 2000 å­˜ä¸€æ¬¡ FAISS
    ner.run(batch_size=4, small_step=100, big_step=2000)